{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Tokenizers - Character based, n-gram, BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\example5\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.1 (tags/v3.10.1:2cd268a, Dec  6 2021, 19:10:37) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 사용한 패키지 버전\\npip 21.2.4\\ntorch 1.11.0+cu113\\ntorchtext 0.6.0\\ntorchdata 0.3.0\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' 사용한 패키지 버전\n",
    "pip 21.2.4\n",
    "torch 1.11.0+cu113\n",
    "torchtext 0.6.0\n",
    "torchdata 0.3.0\n",
    "transformers 4.18.0\n",
    "sentencepiece 0.1.96\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus & Out of Vocabulary (OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '책상', '위에', '사과를', '먹었다']\n"
     ]
    }
   ],
   "source": [
    "s1 = '나는 책상 위에 사과를 먹었다'\n",
    "s2 = '알고 보니 그 사과는 Jason 것이었다'\n",
    "s3 = '그래서 Jason에게 사과를 했다'\n",
    "\n",
    "print(s1.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['알고', '보니', '그', '사과는', 'Jason', '것이었다']\n"
     ]
    }
   ],
   "source": [
    "print(s2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그래서', 'Jason에게', '사과를', '했다']\n"
     ]
    }
   ],
   "source": [
    "print(s3.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', ' ', '책', '상', ' ', '위', '에', ' ', '사', '과', '를', ' ', '먹', '었', '다']\n"
     ]
    }
   ],
   "source": [
    "print(list(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 0, '책상': 1, '위에': 2, '사과를': 3, '먹었다': 4, '알고': 5, '보니': 6, '그': 7, '사과는': 8, 'Jason': 9, '것이었다': 10, '그래서': 11, 'Jason에게': 12, '했다': 13}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {}\n",
    "index = 0\n",
    "\n",
    "for sentence in [s1,s2,s3]:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token2idx.get(token) == None:\n",
    "            token2idx[token] = index\n",
    "            index += 1\n",
    "            \n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "def indexed_sentence(sentence):\n",
    "    return [token2idx[token] for token in sentence]\n",
    "\n",
    "s1_i = indexed_sentence(s1.split())\n",
    "print(s1_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "s2_i = indexed_sentence(s2.split())\n",
    "print(s2_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 3, 13]\n"
     ]
    }
   ],
   "source": [
    "s3_i = indexed_sentence(s3.split())\n",
    "print(s3_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''OOV (Out Of Vocabulary)'''\n",
    "s4 = '나는 책상 위에 배를 먹었다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {t:i+1 for t, i in token2idx.items()}\n",
    "token2idx['<unk>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'나는': 1,\n",
       " '책상': 2,\n",
       " '위에': 3,\n",
       " '사과를': 4,\n",
       " '먹었다': 5,\n",
       " '알고': 6,\n",
       " '보니': 7,\n",
       " '그': 8,\n",
       " '사과는': 9,\n",
       " 'Jason': 10,\n",
       " '것이었다': 11,\n",
       " '그래서': 12,\n",
       " 'Jason에게': 13,\n",
       " '했다': 14,\n",
       " '<unk>': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 0, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def indexed_sentence_unk(sentence):\n",
    "    return [token2idx.get(token,token2idx['<unk>']) for token in sentence]\n",
    "\n",
    "indexed_sentence_unk(s4.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Character based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(65,91)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(97,123)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(32,48)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', ';', '<', '=', '>', '?', '@']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(58,65)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', '\\\\', ']', '^', '_', '`']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(91,97)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(123,127)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(48,58)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가', '각', '갂', '갃', '간', '갅', '갆', '갇', '갈', '갉', '갊', '갋', '갌', '갍', '갎', '갏', '감', '갑', '값', '갓', '갔', '강', '갖', '갗', '갘', '같', '갚', '갛', '개', '객', '갞', '갟', '갠', '갡', '갢', '갣', '갤', '갥', '갦', '갧', '갨', '갩', '갪', '갫', '갬', '갭', '갮', '갯', '갰', '갱']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(int('0xAC00',16), int('0xD7A3',16)+1)][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['뮠', '뮡', '뮢', '뮣', '뮤', '뮥', '뮦', '뮧', '뮨', '뮩', '뮪', '뮫', '뮬', '뮭', '뮮', '뮯', '뮰', '뮱', '뮲', '뮳', '뮴', '뮵', '뮶', '뮷', '뮸', '뮹', '뮺', '뮻', '뮼', '뮽', '뮾', '뮿', '므', '믁', '믂', '믃', '믄', '믅', '믆', '믇', '믈', '믉', '믊', '믋', '믌', '믍', '믎', '믏', '믐', '믑']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(int('0xAC00',16), int('0xD7A3',16)+1)][4000:4050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n"
     ]
    }
   ],
   "source": [
    "print([chr(k) for k in range(int('0x3131',16),int('0x3163',16)+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = {0:'<pad>', 1:'<unk>'}\n",
    "\n",
    "str_idx = len(idx2char)\n",
    "\n",
    "for x in range(32,127):\n",
    "    idx2char.update({str_idx: chr(x)})\n",
    "    str_idx +=1\n",
    "    \n",
    "for x in range(int('0x3131',16),int('0x3163',16)+1):\n",
    "    idx2char.update({str_idx:chr(x)})\n",
    "    str_idx += 1\n",
    "    \n",
    "for x in range(int('0xAC00',16),int('0xD7A3',16)+1):\n",
    "    idx2char.update({str_idx:chr(x)})\n",
    "    str_idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {v:k for k, v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[652, 3116, 5552, 2, 44, 67, 85, 81, 80, 6756, 288, 2, 5440, 400, 3600, 2, 10780, 1912]\n"
     ]
    }
   ],
   "source": [
    "print([char2idx.get(c,0) for c in '그래서 Jason에게 사과를 했다']) #띄어쓰기가 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119, 123, 3, 2, 99, 99]\n"
     ]
    }
   ],
   "source": [
    "print([char2idx.get(c,0) for c in 'ㅇㅋ! ㄳㄳ']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119, 123, 3, 2, 97, 117, 97, 117]\n"
     ]
    }
   ],
   "source": [
    "print([char2idx.get(c,0) for c in 'ㅇㅋ! ㄱㅅㄱㅅ']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. n-gram tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', ' ', '책', '상', ' ', '위', '에', ' ', '사', '과', '를', ' ', '먹', '었', '다']\n"
     ]
    }
   ],
   "source": [
    "s1 = '나는 책상 위에 사과를 먹었다'\n",
    "\n",
    "print([s1[i:i+1] for i in range(len(s1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '는 ', ' 책', '책상', '상 ', ' 위', '위에', '에 ', ' 사', '사과', '과를', '를 ', ' 먹', '먹었', '었다', '다']\n"
     ]
    }
   ],
   "source": [
    "print([s1[i:i+2] for i in range(len(s1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는 ', '는 책', ' 책상', '책상 ', '상 위', ' 위에', '위에 ', '에 사', ' 사과', '사과를', '과를 ', '를 먹', ' 먹었', '먹었다', '었다', '다']\n"
     ]
    }
   ],
   "source": [
    "print([s1[i:i+3] for i in range(len(s1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am', 'dying'], ['am', 'dying', 'to'], ['dying', 'to', 'play'], ['to', 'play', 'the'], ['play', 'the', 'game'], ['the', 'game'], ['game']]\n"
     ]
    }
   ],
   "source": [
    "#띄어쓰기 기준에 적용\n",
    "\n",
    "s2 = 'I am dying to play the game'\n",
    "\n",
    "print([s2.split()[i:i+3] for i in range(len(s2.split()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['너', '때문에', '간'], ['때문에', '간', '떨어질'], ['간', '떨어질', '뻔했다'], ['떨어질', '뻔했다'], ['뻔했다']]\n"
     ]
    }
   ],
   "source": [
    "#띄어쓰기 기준에 적용\n",
    "\n",
    "s3 = '너 때문에 간 떨어질 뻔했다'\n",
    "\n",
    "print([s3.split()[i:i+3] for i in range(len(s3.split()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BPE(Byte Pair Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Algorithm : Learn BEP operations'''\n",
    "\n",
    "import re, collections\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "            \n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\\\s)'+bigram+r'(?!\\\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair),word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "('e', 's')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "\\n\n",
      "Step 2\n",
      "('es', 't')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "\\n\n",
      "Step 3\n",
      "('est', '</w>')\n",
      "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\\n\n",
      "Step 4\n",
      "('l', 'o')\n",
      "{'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\\n\n",
      "Step 5\n",
      "('lo', 'w')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "\\n\n",
      "Step 6\n",
      "('n', 'e')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
      "\\n\n",
      "Step 7\n",
      "('ne', 'w')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
      "\\n\n",
      "Step 8\n",
      "('new', 'est</w>')\n",
      "{'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "\\n\n",
      "Step 9\n",
      "('low', '</w>')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "\\n\n",
      "Step 10\n",
      "('w', 'i')\n",
      "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n",
      "\\n\n"
     ]
    }
   ],
   "source": [
    "vocab = {'l o w </w>':5, 'l o w e r </w>' : 2,\n",
    "        'n e w e s t </w>':6, 'w i d e s t </w>':3}\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f'Step {i+1}')\n",
    "    print(best)\n",
    "    print(vocab)\n",
    "    print('\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '나는 책상 위에 사과를 먹었다'\n",
    "s2 = '알고 보니 그 사과는 Jason 것이었다'\n",
    "s3 = '그래서 Jason에게 사과를 했다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사 과 를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사 과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n"
     ]
    }
   ],
   "source": [
    "token_counts = {}\n",
    "index = 0\n",
    "\n",
    "for sentence in [s1,s2,s3]:\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token_counts.get(token) == None:\n",
    "            token_counts[token] = 1\n",
    "        else:\n",
    "            token_counts[token] +=1\n",
    "            \n",
    "token_counts = {\" \".join(token):counts for token, counts in token_counts.items()}\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "('사', '과')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과 를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 2\n",
      "('사과', '를')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었 다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었 다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 3\n",
      "('었', '다')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'J a s o n': 1, '것 이 었다': 1, '그 래 서': 1, 'J a s o n 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 4\n",
      "('J', 'a')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Ja s o n': 1, '것 이 었다': 1, '그 래 서': 1, 'Ja s o n 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 5\n",
      "('Ja', 's')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jas o n': 1, '것 이 었다': 1, '그 래 서': 1, 'Jas o n 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 6\n",
      "('Jas', 'o')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jaso n': 1, '것 이 었다': 1, '그 래 서': 1, 'Jaso n 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 7\n",
      "('Jaso', 'n')\n",
      "{'나 는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 8\n",
      "('나', '는')\n",
      "{'나는': 1, '책 상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 9\n",
      "('책', '상')\n",
      "{'나는': 1, '책상': 1, '위 에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\\n\n",
      "Step 10\n",
      "('위', '에')\n",
      "{'나는': 1, '책상': 1, '위에': 1, '사과를': 2, '먹 었다': 1, '알 고': 1, '보 니': 1, '그': 1, '사과 는': 1, 'Jason': 1, '것 이 었다': 1, '그 래 서': 1, 'Jason 에 게': 1, '했 다': 1}\n",
      "\\n\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(token_counts)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    token_counts = merge_vocab(best, token_counts)\n",
    "    print(f'Step {i+1}')\n",
    "    print(best)\n",
    "    print(token_counts)\n",
    "    print('\\\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Trained Tokenizer 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "#s = spm.SentencePieceProcessor(model_file = 'spm.model')\n",
    "#for n in range(5):\n",
    "#    s.encode('New York', out_type=str, enable_sampling = True, alpha = 0.1, nbest = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 226k/226k [00:00<00:00, 286kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 5.59kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 145kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cute', '.', 'he', 'likes', 'playing']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence = 'My dog is cute. He likes playing'\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 851k/851k [00:01<00:00, 586kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 5.61kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 625/625 [00:00<00:00, 157kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cut', '##e', '.', 'he', 'likes', 'playing']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '나는 책상 위에 사과를 먹었다. 알고 보니 그 사과는 Jason 것이었다. 그래서 Jason에게 사과를 했다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', 'ᄎ', '##ᅢᆨ', '##상', '위에', 'ᄉ', '##ᅡ', '##과', '##를', 'ᄆ', '##ᅥ', '##ᆨ', '##었다', '.', '알', '##고', 'ᄇ', '##ᅩ', '##니', '그', 'ᄉ', '##ᅡ', '##과', '##는', 'jason', '것이', '##었다', '.', '그', '##래', '##서', 'jason', '##에게', 'ᄉ', '##ᅡ', '##과', '##를', '했다']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example5",
   "language": "python",
   "name": "example5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
